{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"METMQlGHlTKn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748506427442,"user_tz":-120,"elapsed":34557,"user":{"displayName":"Gruppo ML","userId":"14938435567714989624"}},"outputId":"2025c55e-da00-421f-aa6c-fe5f03c4c9b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting trimesh\n","  Downloading trimesh-4.6.10-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from trimesh) (2.0.2)\n","Downloading trimesh-4.6.10-py3-none-any.whl (711 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m711.2/711.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: trimesh\n","Successfully installed trimesh-4.6.10\n","Mounted at /content/drive\n"]}],"source":["import os\n","import yaml\n","import torch\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from google.colab import drive\n","import cv2\n","import torch.nn as nn\n","import torchvision.models as models\n","import random\n","from torchvision.models import resnet50, ResNet50_Weights\n","!pip install trimesh\n","import trimesh\n","\n","def compute_add(model_points, gt_R, gt_t, pred_R, pred_t):\n","    batch_size = pred_R.size(0)\n","    add_losses = []\n","\n","    for b in range(batch_size):\n","        pts = model_points[b].to(pred_R.device)  # [N, 3]\n","\n","        pred_transformed = pred_R[b] @ pts.T + pred_t[b][:, None]  # [3, N]\n","        gt_transformed = gt_R[b] @ pts.T + gt_t[b][:, None]        # [3, N]\n","\n","        dist = torch.norm(pred_transformed - gt_transformed, dim=0)  # [N]\n","        add_loss_b = dist.mean()\n","        add_losses.append(add_loss_b)\n","\n","    return torch.stack(add_losses).mean()\n","\n","def load_ply_vertices(ply_path):\n","    mesh = trimesh.load(ply_path)\n","    return mesh.vertices.astype(np.float32)\n","\n","def set_seed(seed=42):\n","    random.seed(seed)  # Python\n","    np.random.seed(seed)  # NumPy\n","    torch.manual_seed(seed)  # CPU\n","    torch.cuda.manual_seed(seed)  # GPU singola\n","    torch.cuda.manual_seed_all(seed)  # Tutte le GPU\n","\n","    # Comportamento deterministico per reproducibilità\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def normalize_quaternion(q):\n","    return q / q.norm(dim=-1, keepdim=True)\n","\n","def quaternion_to_matrix(q):\n","    # Assumiamo q shape: [B, 4] → ritorna [B, 3, 3]\n","    x, y, z, w = q.unbind(-1)\n","\n","    B = q.size(0)\n","    R = torch.empty(B, 3, 3, device=q.device, dtype=q.dtype)\n","    R[:, 0, 0] = 1 - 2*(y*y + z*z)\n","    R[:, 0, 1] = 2*(x*y - z*w)\n","    R[:, 0, 2] = 2*(x*z + y*w)\n","    R[:, 1, 0] = 2*(x*y + z*w)\n","    R[:, 1, 1] = 1 - 2*(x*x + z*z)\n","    R[:, 1, 2] = 2*(y*z - x*w)\n","    R[:, 2, 0] = 2*(x*z - y*w)\n","    R[:, 2, 1] = 2*(y*z + x*w)\n","    R[:, 2, 2] = 1 - 2*(x*x + y*y)\n","    return R\n","\n","def geodesic_loss(R_pred, R_gt):\n","    # R_pred, R_gt: [B, 3, 3]\n","    R_diff = torch.bmm(R_pred.transpose(1, 2), R_gt)\n","    trace = R_diff[:, 0, 0] + R_diff[:, 1, 1] + R_diff[:, 2, 2]  # batch trace\n","    cos_theta = (trace - 1) / 2\n","    cos_theta = torch.clamp(cos_theta, -1 + 1e-6, 1 - 1e-6)  # stabilità numerica\n","    theta = torch.acos(cos_theta)\n","    return torch.mean(theta)\n","\n","class PreprocessedPoseDataset_RGBD(Dataset):\n","    def __init__(self, filenames, rgb_images, depth_images, pose_data, mean_t, std_t):\n","        self.rgb_images = rgb_images\n","        self.pose_data = pose_data\n","        self.depth_images = depth_images\n","        self.filenames = filenames\n","        self.mean_t = mean_t\n","        self.std_t = std_t\n","\n","        self.obj_id_to_idx = {\n","            1: 0, 2: 1, 4: 2, 5: 3, 6: 4,\n","            8: 5, 9: 6, 10: 7, 11: 8, 12: 9,\n","            13: 10, 14: 11, 15: 12\n","        }\n","\n","    def __len__(self):\n","        return len(self.rgb_images)\n","\n","    def __getitem__(self, idx):\n","        filename = self.filenames[idx]\n","        name_key = os.path.splitext(filename)[0]\n","\n","        rgb = self.rgb_images[idx]\n","        depth = self.depth_images[idx]\n","        sample = self.pose_data[name_key][0]\n","\n","        rotation = torch.tensor(sample['cam_R_m2c'], dtype=torch.float32).view(3, 3)\n","        translation = torch.tensor(sample['cam_t_m2c'], dtype=torch.float32)\n","        translation = (translation - self.mean_t) / self.std_t  # Normalizzazione\n","\n","        obj_id_raw = sample['obj_id']\n","        obj_id = torch.tensor(self.obj_id_to_idx[obj_id_raw], dtype=torch.long)\n","\n","        return {\n","            'image': rgb,\n","            'depth_map': depth,\n","            'cam_R_m2c': rotation,\n","            'cam_t_m2c': translation,\n","            'obj_id': obj_id\n","        }\n","\n","class PoseRegressor_RGBD(nn.Module):\n","    def __init__(self, num_obj_ids, embedding_dim=32, use_learned_default=True):\n","        super(PoseRegressor_RGBD, self).__init__()\n","\n","        # Feature extractor RGB (pre-trained)\n","        resnet_rgb = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n","        self.rgb_extractor = nn.Sequential(*list(resnet_rgb.children())[:-1])  # [B, 2048, 1, 1]\n","\n","        # Feature extractor depth (lighter, from scratch or pre-trained on grayscale)\n","        resnet_depth = models.resnet18(weights=None)\n","        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.depth_extractor = nn.Sequential(*list(resnet_depth.children())[:-1])  # [B, 512, 1, 1]\n","\n","        self.obj_id_embedding = nn.Embedding(num_obj_ids, embedding_dim)\n","\n","        # Embedding predefinito (se richiesto)\n","        self.use_learned_default = use_learned_default\n","        if use_learned_default:\n","            self.default_obj_embedding = nn.Parameter(torch.zeros(embedding_dim))\n","\n","        self.fc_common = nn.Sequential(\n","            nn.Linear(2048 + embedding_dim + 512, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(512, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU())\n","\n","        self.fc_translation = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 3))\n","\n","        self.fc_rotation = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 4))\n","\n","    def forward(self, rgb, depth, obj_id=None):\n","        feat_rgb = self.rgb_extractor(rgb)       # [B, 2048, 1, 1]\n","        feat_depth = self.depth_extractor(depth) # [B, 512, 1, 1]\n","\n","        feat_rgb = feat_rgb.view(feat_rgb.size(0), -1)       # [B, 2048]\n","        feat_depth = feat_depth.view(feat_depth.size(0), -1) # [B, 512]\n","\n","        if obj_id is not None:\n","            obj_embed = self.obj_id_embedding(obj_id)  # [B, D]\n","        else:\n","            if self.use_learned_default:\n","                obj_embed = self.default_obj_embedding.unsqueeze(0).expand(rgb.size(0), -1)  # [B, D]\n","            else:\n","                obj_embed = torch.zeros(x.size(0), self.obj_id_embedding.embedding_dim, device=rgb.device)\n","\n","        features = torch.cat([feat_rgb, feat_depth], dim=1)\n","        x = torch.cat([features, obj_embed], dim=1)  # [B, 2048 + D]\n","        x = self.fc_common(x)\n","        translation = self.fc_translation(x)\n","        rotation_q = self.fc_rotation(x)  # [B, 4]\n","        rotation_q = normalize_quaternion(rotation_q)  # normalizza il quaternione\n","        rotation = quaternion_to_matrix(rotation_q)     # converte in matrice 3x3\n","        return translation, rotation\n","\n","if torch.cuda.is_available():\n","  device = \"cuda\"\n","else:\n","  device = \"cpu\"\n","\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["set_seed(42)\n","pose_file = \"/content/drive/MyDrive/DatasetCorrect/gt_all_new.yml\" # Indirizzo della ground truth\n","img_dir = \"/content/drive/MyDrive/DatasetCorrect/dataset_tensors/tensors_rgb.pt\"\n","img_depth_dir = \"/content/drive/MyDrive/DatasetCorrect/dataset_tensors/tensors_dpt.pt\" # Indirizzo di depth dell'analizzato\n","\n","id_dataset_to_ordered = {1: 0, 2: 1, 4: 2, 5: 3, 6: 4,\n","            8: 5, 9: 6, 10: 7, 11: 8, 12: 9,\n","            13: 10, 14: 11, 15: 12}\n","\n","models_points = {}\n","\n","for i in range(15):\n","  if i != 2 and i != 6:\n","    pts = (load_ply_vertices(f\"/content/drive/MyDrive/DatasetCorrect/normalized_models/obj_norm_{i+1:02d}.ply\"))\n","    models_points[id_dataset_to_ordered[i+1]] = torch.tensor(pts, dtype=torch.float32).to(device)\n","\n","with open(pose_file, 'r') as f:\n","  pose_data = yaml.load(f, Loader=yaml.FullLoader)\n","\n","def compute_translation_stats(pose_data):\n","    translations = []\n","    for v in pose_data.values():\n","        t = np.array(v[0]['cam_t_m2c'])\n","        translations.append(t)\n","    translations = np.stack(translations)\n","    mean = translations.mean(axis=0)\n","    std = translations.std(axis=0)\n","    return mean, std\n","\n","mean_t, std_t = compute_translation_stats(pose_data)\n","mean_t = torch.tensor(mean_t, dtype=torch.float32)\n","std_t = torch.tensor(std_t, dtype=torch.float32)\n","\n","## Creating DataLoader\n","\n","filenames, rgb_tensors = torch.load(img_dir)  # Carica tutti i tensori RGB\n","\n","with open(\"/content/drive/MyDrive/DatasetCorrect/dataset_indexes/train_indexes.txt\", \"r\") as f:\n","    valid_names = set(line.strip() for line in f if line.strip())\n","\n","with open(\"/content/drive/MyDrive/DatasetCorrect/dataset_indexes/val_indexes.txt\", \"r\") as f:\n","    valid_names_val = set(line.strip() for line in f if line.strip())\n","\n","tr_filenames = []\n","val_filenames = []\n","\n","tr_rgb_tensors = []\n","val_rgb_tensors = []\n","\n","for fname, tensor in zip(filenames, rgb_tensors):\n","    name_no_ext = os.path.splitext(fname)[0]\n","    if name_no_ext in valid_names:\n","        tr_filenames.append(fname)\n","        tr_rgb_tensors.append(tensor)\n","    elif name_no_ext in valid_names_val:\n","        val_filenames.append(fname)\n","        val_rgb_tensors.append(tensor)\n","\n","torch.cuda.empty_cache()\n","\n","depth_tensors = torch.load(img_depth_dir)  # Carica tutti i tensori di profondità\n","\n","tr_dpt_tensors = []\n","val_dpt_tensors = []\n","\n","for fname, tensor in zip(filenames, depth_tensors):\n","    name_no_ext = os.path.splitext(fname)[0]\n","    if name_no_ext in tr_filenames:\n","        tr_dpt_tensors.append(tensor)\n","    elif name_no_ext in val_filenames:\n","        val_dpt_tensors.append(tensor)\n","\n","del filenames\n","torch.cuda.empty_cache()\n","\n","tr_dataset = PreprocessedPoseDataset_RGBD(tr_filenames, tr_rgb_tensors, tr_dpt_tensors, pose_data, mean_t, std_t)\n","val_dataset = PreprocessedPoseDataset_RGBD(val_filenames, val_rgb_tensors, val_dpt_tensors, pose_data, mean_t, std_t)\n","\n","tr_dataloader = DataLoader(tr_dataset, batch_size=8, shuffle=True, num_workers=2, drop_last = True)\n","val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, drop_last = True)"],"metadata":{"id":"vKUWnVuMmOK6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_seed(42)\n","## ATTENZIONE: PREDIZIONI NORMALIZZATE, VANNO POI RIPORTATE IN MONDO \"REALE\"\n","LR = 0.000000001\n","\n","model = PoseRegressor_RGBD(num_obj_ids=13).to(device)\n","\n","alpha = 0.01 # Translation Loss coefficient\n","beta = 0.01 # Rotation Loss coefficient\n","gamma = 1 # ADD Loss Coefficient\n","beta_t_lossL1smooth = 1\n","\n","lr_backbone_rgb = 0.000000001\n","lr_backbone_dpt = 0.000000001\n","lr_common = 0.000000001\n","lr_translation = 0.000000001\n","lr_rotation = 0.000000005\n","\n","optimizer = torch.optim.Adam([\n","    {\"params\": model.rgb_extractor.parameters(), \"lr\": lr_backbone_rgb},\n","    {\"params\": model.depth_extractor.parameters(), \"lr\": lr_backbone_dpt},\n","    {\"params\": model.fc_common.parameters(), \"lr\": lr_common},\n","    {\"params\": model.fc_translation.parameters(), \"lr\": lr_translation},\n","    {\"params\": model.fc_rotation.parameters(), \"lr\": lr_rotation},], lr=LR)\n","\n","posemodelidrgb = 189 # Scegliere versione modello PoseModel RGB da cui partire\n","n_epochs = 30 # Numero di epoche da trainare\n","\n","# Per ogni parametro del modello, assicurati che i gradienti siano abilitati\n","for param in model.parameters():\n","    param.requires_grad = True\n","\n","if posemodelidrgb > 0:\n","  checkpoint = torch.load(f\"/content/drive/MyDrive/RGB+DEPTH_PoseModels_Correct/posemodel_rgb+dpt_{posemodelidrgb}.pth\")\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","optimizer.param_groups[0]['lr'] = lr_backbone_rgb\n","optimizer.param_groups[1]['lr'] = lr_backbone_dpt\n","optimizer.param_groups[2]['lr'] = lr_common\n","optimizer.param_groups[3]['lr'] = lr_translation\n","optimizer.param_groups[4]['lr'] = lr_rotation\n","\n","for i in range(n_epochs):\n","  model.train()\n","\n","  total_loss = 0.0\n","  total_t_loss = 0.0\n","  total_r_loss = 0.0\n","  total_samples = 0\n","  total_add = 0.0\n","\n","  for batch in tr_dataloader:\n","      batch_size = batch['image'].size(0)\n","      total_samples += batch_size\n","\n","      images = batch['image'].to(device)                 # [B, 3, 224, 224]\n","      depth_maps = batch['depth_map'].to(device)\n","      gt_t = batch['cam_t_m2c'].to(device).float()               # [B, 3]\n","      gt_r = batch['cam_R_m2c'].to(device).float()               # [B, 3, 3]\n","      obj_ids = batch['obj_id'].to(device)               # [B]\n","\n","      pred_t, pred_r = model(images, depth_maps, obj_ids)\n","\n","      model_points_batch = [models_points[obj_ids[j].item()] for j in range(batch_size)]\n","\n","      t_loss = nn.functional.smooth_l1_loss(pred_t, gt_t, beta=beta_t_lossL1smooth, reduction='mean')\n","      #t_loss = nn.functional.mse_loss(pred_t, gt_t, reduction='mean')\n","      r_loss = geodesic_loss(pred_r, gt_r)\n","      add_loss = compute_add(model_points_batch, gt_r, gt_t, pred_r, pred_t)\n","      loss = gamma * add_loss + alpha * t_loss + beta * r_loss\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      total_loss += loss.item() * batch_size\n","      total_t_loss += t_loss.item() * batch_size\n","      total_r_loss += r_loss.item() * batch_size\n","      total_add += add_loss.item() * batch_size\n","\n","  avg_loss = total_loss / total_samples\n","  avg_rot_loss = total_r_loss / total_samples\n","  avg_trans_loss = total_t_loss / total_samples\n","  avg_add = total_add / total_samples\n","\n","  # VALIDATION\n","  model.eval()\n","  val_total_loss = 0.0\n","  val_total_t_loss = 0.0\n","  val_total_r_loss = 0.0\n","  val_total_samples = 0\n","  val_total_add = 0.0\n","\n","  with torch.no_grad():\n","      for val_batch in val_dataloader:\n","          batch_size = val_batch['image'].size(0)\n","          val_total_samples += batch_size\n","\n","          images = val_batch['image'].to(device)\n","          depth_maps = batch['depth_map'].to(device)\n","          gt_t = val_batch['cam_t_m2c'].to(device).float()\n","          gt_r = val_batch['cam_R_m2c'].to(device).float()\n","          obj_ids = val_batch['obj_id'].to(device)\n","\n","          pred_t, pred_r = model(images,depth_maps, obj_ids)\n","\n","          model_points_batch = [models_points[obj_ids[j].item()] for j in range(batch_size)]\n","\n","          t_loss = nn.functional.smooth_l1_loss(pred_t, gt_t, beta=beta_t_lossL1smooth, reduction='mean')\n","          r_loss = geodesic_loss(pred_r, gt_r)\n","          add_loss = compute_add(model_points_batch, gt_r, gt_t, pred_r, pred_t)\n","          loss = gamma * add_loss + alpha * t_loss + beta * r_loss\n","\n","          val_total_loss += loss.item() * batch_size\n","          val_total_t_loss += t_loss.item() * batch_size\n","          val_total_r_loss += r_loss.item() * batch_size\n","          val_total_add += add_loss.item() * batch_size\n","\n","  val_avg_loss = val_total_loss / val_total_samples\n","  val_avg_rot_loss = val_total_r_loss / val_total_samples\n","  val_avg_trans_loss = val_total_t_loss / val_total_samples\n","  val_avg_add = val_total_add / val_total_samples\n","\n","  torch.save({'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, f\"/content/drive/MyDrive/RGB+DEPTH_PoseModels_Correct/posemodel_rgb+dpt_{posemodelidrgb+i+1}.pth\")\n","  print(f\"{i+1}/{n_epochs} -> - Epoch {posemodelidrgb+i+1}\")\n","  print(f\"Average Training ADD (per Sample): {avg_add:.4f} | Avg. Rot. Loss: {avg_rot_loss:.4f} rad / {avg_rot_loss*(180 / torch.pi):.4f}° | Avg. Trans. Loss: {avg_trans_loss:.4f}\" )\n","  print(f\"Average Validation ADD (per Sample): {val_avg_loss:.4f} | Avg. Rot. Loss: {val_avg_rot_loss:.4f} rad / {val_avg_rot_loss*(180 / torch.pi):.4f}° | Avg. Trans. Loss: {val_avg_trans_loss:.4f}\")\n","  print(\"-----------------------------------------------------------------------\")"],"metadata":{"id":"Ej_VrhKWmxDL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad5cfc01-42ba-4cc1-f504-4e0e81803979","executionInfo":{"status":"ok","timestamp":1748550853033,"user_tz":-120,"elapsed":4025034,"user":{"displayName":"Gruppo ML","userId":"14938435567714989624"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/30 -> - Epoch 190\n","Average Training ADD (per Sample): 1.4705 | Avg. Rot. Loss: 0.7592 rad / 43.4992° | Avg. Trans. Loss: 0.2211\n","Average Validation ADD (per Sample): 2.0414 | Avg. Rot. Loss: 0.7826 rad / 44.8388° | Avg. Trans. Loss: 0.4810\n","-----------------------------------------------------------------------\n","2/30 -> - Epoch 191\n","Average Training ADD (per Sample): 1.4699 | Avg. Rot. Loss: 0.7570 rad / 43.3716° | Avg. Trans. Loss: 0.2208\n","Average Validation ADD (per Sample): 2.0460 | Avg. Rot. Loss: 0.8087 rad / 46.3351° | Avg. Trans. Loss: 0.4851\n","-----------------------------------------------------------------------\n","3/30 -> - Epoch 192\n","Average Training ADD (per Sample): 1.4575 | Avg. Rot. Loss: 0.7471 rad / 42.8069° | Avg. Trans. Loss: 0.2192\n","Average Validation ADD (per Sample): 2.1372 | Avg. Rot. Loss: 0.7736 rad / 44.3212° | Avg. Trans. Loss: 0.5358\n","-----------------------------------------------------------------------\n","4/30 -> - Epoch 193\n","Average Training ADD (per Sample): 1.4658 | Avg. Rot. Loss: 0.7626 rad / 43.6925° | Avg. Trans. Loss: 0.2186\n","Average Validation ADD (per Sample): 2.1102 | Avg. Rot. Loss: 0.7756 rad / 44.4412° | Avg. Trans. Loss: 0.5224\n","-----------------------------------------------------------------------\n","5/30 -> - Epoch 194\n","Average Training ADD (per Sample): 1.4476 | Avg. Rot. Loss: 0.7453 rad / 42.7018° | Avg. Trans. Loss: 0.2164\n","Average Validation ADD (per Sample): 2.0995 | Avg. Rot. Loss: 0.7963 rad / 45.6268° | Avg. Trans. Loss: 0.5064\n","-----------------------------------------------------------------------\n","6/30 -> - Epoch 195\n","Average Training ADD (per Sample): 1.4511 | Avg. Rot. Loss: 0.7458 rad / 42.7329° | Avg. Trans. Loss: 0.2179\n","Average Validation ADD (per Sample): 2.0544 | Avg. Rot. Loss: 0.7745 rad / 44.3782° | Avg. Trans. Loss: 0.4964\n","-----------------------------------------------------------------------\n","7/30 -> - Epoch 196\n","Average Training ADD (per Sample): 1.4630 | Avg. Rot. Loss: 0.7556 rad / 43.2950° | Avg. Trans. Loss: 0.2187\n","Average Validation ADD (per Sample): 2.0343 | Avg. Rot. Loss: 0.7692 rad / 44.0728° | Avg. Trans. Loss: 0.4889\n","-----------------------------------------------------------------------\n","8/30 -> - Epoch 197\n","Average Training ADD (per Sample): 1.4808 | Avg. Rot. Loss: 0.7613 rad / 43.6218° | Avg. Trans. Loss: 0.2255\n","Average Validation ADD (per Sample): 2.1012 | Avg. Rot. Loss: 0.7566 rad / 43.3502° | Avg. Trans. Loss: 0.5232\n","-----------------------------------------------------------------------\n","9/30 -> - Epoch 198\n","Average Training ADD (per Sample): 1.4869 | Avg. Rot. Loss: 0.7590 rad / 43.4868° | Avg. Trans. Loss: 0.2284\n","Average Validation ADD (per Sample): 2.0572 | Avg. Rot. Loss: 0.7683 rad / 44.0197° | Avg. Trans. Loss: 0.4993\n","-----------------------------------------------------------------------\n","10/30 -> - Epoch 199\n","Average Training ADD (per Sample): 1.4858 | Avg. Rot. Loss: 0.7638 rad / 43.7631° | Avg. Trans. Loss: 0.2251\n","Average Validation ADD (per Sample): 2.0304 | Avg. Rot. Loss: 0.7488 rad / 42.9037° | Avg. Trans. Loss: 0.4922\n","-----------------------------------------------------------------------\n","11/30 -> - Epoch 200\n","Average Training ADD (per Sample): 1.4771 | Avg. Rot. Loss: 0.7517 rad / 43.0664° | Avg. Trans. Loss: 0.2245\n","Average Validation ADD (per Sample): 2.1463 | Avg. Rot. Loss: 0.7788 rad / 44.6220° | Avg. Trans. Loss: 0.5453\n","-----------------------------------------------------------------------\n","12/30 -> - Epoch 201\n","Average Training ADD (per Sample): 1.4847 | Avg. Rot. Loss: 0.7691 rad / 44.0687° | Avg. Trans. Loss: 0.2243\n","Average Validation ADD (per Sample): 2.0488 | Avg. Rot. Loss: 0.7729 rad / 44.2821° | Avg. Trans. Loss: 0.4967\n","-----------------------------------------------------------------------\n","13/30 -> - Epoch 202\n","Average Training ADD (per Sample): 1.4949 | Avg. Rot. Loss: 0.7722 rad / 44.2460° | Avg. Trans. Loss: 0.2267\n","Average Validation ADD (per Sample): 2.1288 | Avg. Rot. Loss: 0.8056 rad / 46.1558° | Avg. Trans. Loss: 0.5222\n","-----------------------------------------------------------------------\n","14/30 -> - Epoch 203\n","Average Training ADD (per Sample): 1.4922 | Avg. Rot. Loss: 0.7681 rad / 44.0063° | Avg. Trans. Loss: 0.2295\n","Average Validation ADD (per Sample): 2.0944 | Avg. Rot. Loss: 0.7590 rad / 43.4880° | Avg. Trans. Loss: 0.5223\n","-----------------------------------------------------------------------\n","15/30 -> - Epoch 204\n","Average Training ADD (per Sample): 1.4829 | Avg. Rot. Loss: 0.7633 rad / 43.7366° | Avg. Trans. Loss: 0.2236\n","Average Validation ADD (per Sample): 2.0628 | Avg. Rot. Loss: 0.7707 rad / 44.1588° | Avg. Trans. Loss: 0.5024\n","-----------------------------------------------------------------------\n","16/30 -> - Epoch 205\n","Average Training ADD (per Sample): 1.4906 | Avg. Rot. Loss: 0.7667 rad / 43.9279° | Avg. Trans. Loss: 0.2283\n","Average Validation ADD (per Sample): 2.1431 | Avg. Rot. Loss: 0.7717 rad / 44.2165° | Avg. Trans. Loss: 0.5396\n","-----------------------------------------------------------------------\n","17/30 -> - Epoch 206\n","Average Training ADD (per Sample): 1.4659 | Avg. Rot. Loss: 0.7553 rad / 43.2741° | Avg. Trans. Loss: 0.2199\n","Average Validation ADD (per Sample): 2.0419 | Avg. Rot. Loss: 0.7581 rad / 43.4339° | Avg. Trans. Loss: 0.4990\n","-----------------------------------------------------------------------\n","18/30 -> - Epoch 207\n","Average Training ADD (per Sample): 1.5399 | Avg. Rot. Loss: 0.7970 rad / 45.6651° | Avg. Trans. Loss: 0.2370\n","Average Validation ADD (per Sample): 2.0554 | Avg. Rot. Loss: 0.7654 rad / 43.8518° | Avg. Trans. Loss: 0.5001\n","-----------------------------------------------------------------------\n","19/30 -> - Epoch 208\n","Average Training ADD (per Sample): 1.5280 | Avg. Rot. Loss: 0.7799 rad / 44.6857° | Avg. Trans. Loss: 0.2381\n","Average Validation ADD (per Sample): 2.0891 | Avg. Rot. Loss: 0.7884 rad / 45.1716° | Avg. Trans. Loss: 0.5052\n","-----------------------------------------------------------------------\n","20/30 -> - Epoch 209\n","Average Training ADD (per Sample): 1.5324 | Avg. Rot. Loss: 0.7845 rad / 44.9505° | Avg. Trans. Loss: 0.2404\n","Average Validation ADD (per Sample): 2.0944 | Avg. Rot. Loss: 0.7648 rad / 43.8219° | Avg. Trans. Loss: 0.5155\n","-----------------------------------------------------------------------\n","21/30 -> - Epoch 210\n","Average Training ADD (per Sample): 1.5583 | Avg. Rot. Loss: 0.8020 rad / 45.9514° | Avg. Trans. Loss: 0.2443\n","Average Validation ADD (per Sample): 2.1189 | Avg. Rot. Loss: 0.7980 rad / 45.7194° | Avg. Trans. Loss: 0.5202\n","-----------------------------------------------------------------------\n","22/30 -> - Epoch 211\n","Average Training ADD (per Sample): 1.5478 | Avg. Rot. Loss: 0.7996 rad / 45.8156° | Avg. Trans. Loss: 0.2410\n","Average Validation ADD (per Sample): 2.2143 | Avg. Rot. Loss: 0.8054 rad / 46.1484° | Avg. Trans. Loss: 0.5670\n","-----------------------------------------------------------------------\n","23/30 -> - Epoch 212\n","Average Training ADD (per Sample): 1.5581 | Avg. Rot. Loss: 0.8088 rad / 46.3388° | Avg. Trans. Loss: 0.2424\n","Average Validation ADD (per Sample): 2.0832 | Avg. Rot. Loss: 0.7564 rad / 43.3396° | Avg. Trans. Loss: 0.5125\n","-----------------------------------------------------------------------\n","24/30 -> - Epoch 213\n","Average Training ADD (per Sample): 1.5373 | Avg. Rot. Loss: 0.7850 rad / 44.9793° | Avg. Trans. Loss: 0.2399\n","Average Validation ADD (per Sample): 2.0974 | Avg. Rot. Loss: 0.7840 rad / 44.9177° | Avg. Trans. Loss: 0.5128\n","-----------------------------------------------------------------------\n","25/30 -> - Epoch 214\n","Average Training ADD (per Sample): 1.5596 | Avg. Rot. Loss: 0.8095 rad / 46.3824° | Avg. Trans. Loss: 0.2425\n","Average Validation ADD (per Sample): 2.0915 | Avg. Rot. Loss: 0.7703 rad / 44.1340° | Avg. Trans. Loss: 0.5128\n","-----------------------------------------------------------------------\n","26/30 -> - Epoch 215\n","Average Training ADD (per Sample): 1.5329 | Avg. Rot. Loss: 0.7893 rad / 45.2235° | Avg. Trans. Loss: 0.2352\n","Average Validation ADD (per Sample): 2.0707 | Avg. Rot. Loss: 0.7538 rad / 43.1882° | Avg. Trans. Loss: 0.5121\n","-----------------------------------------------------------------------\n","27/30 -> - Epoch 216\n","Average Training ADD (per Sample): 1.5607 | Avg. Rot. Loss: 0.8119 rad / 46.5193° | Avg. Trans. Loss: 0.2421\n","Average Validation ADD (per Sample): 2.1272 | Avg. Rot. Loss: 0.7619 rad / 43.6515° | Avg. Trans. Loss: 0.5399\n","-----------------------------------------------------------------------\n","28/30 -> - Epoch 217\n","Average Training ADD (per Sample): 1.5655 | Avg. Rot. Loss: 0.8004 rad / 45.8617° | Avg. Trans. Loss: 0.2460\n","Average Validation ADD (per Sample): 2.0587 | Avg. Rot. Loss: 0.7554 rad / 43.2834° | Avg. Trans. Loss: 0.5016\n","-----------------------------------------------------------------------\n","29/30 -> - Epoch 218\n","Average Training ADD (per Sample): 1.5670 | Avg. Rot. Loss: 0.8021 rad / 45.9578° | Avg. Trans. Loss: 0.2484\n","Average Validation ADD (per Sample): 2.0901 | Avg. Rot. Loss: 0.7748 rad / 44.3923° | Avg. Trans. Loss: 0.5149\n","-----------------------------------------------------------------------\n","30/30 -> - Epoch 219\n","Average Training ADD (per Sample): 1.5591 | Avg. Rot. Loss: 0.8084 rad / 46.3188° | Avg. Trans. Loss: 0.2440\n","Average Validation ADD (per Sample): 2.0982 | Avg. Rot. Loss: 0.7620 rad / 43.6620° | Avg. Trans. Loss: 0.5185\n","-----------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["print(\"✅ Addestramento completato. Puoi disconnettere il runtime.\")\n","os.kill(os.getpid(), 9)"],"metadata":{"id":"TLmcdsZsfIuC"},"execution_count":null,"outputs":[]}]}