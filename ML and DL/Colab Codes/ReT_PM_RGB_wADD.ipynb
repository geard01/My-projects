{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11_qY3U_0EbHvxf92n566qig9S2ffDGGQ","timestamp":1747812467610}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aOUHvzmvVaNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748355500321,"user_tz":-120,"elapsed":38934,"user":{"displayName":"Gruppo ML","userId":"14938435567714989624"}},"outputId":"65ce4b80-d859-4e81-df57-6d3558afc6fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting trimesh\n","  Downloading trimesh-4.6.10-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from trimesh) (2.0.2)\n","Downloading trimesh-4.6.10-py3-none-any.whl (711 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/711.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m711.2/711.2 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: trimesh\n","Successfully installed trimesh-4.6.10\n","Mounted at /content/drive\n"]}],"source":["import os\n","import yaml\n","import torch\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from google.colab import drive\n","import cv2\n","import torch.nn as nn\n","import torchvision.models as models\n","import random\n","from torchvision.models import resnet50, ResNet50_Weights\n","!pip install trimesh\n","import trimesh\n","\n","def compute_add(model_points, gt_R, gt_t, pred_R, pred_t):\n","    batch_size = pred_R.size(0)\n","    add_losses = []\n","\n","    for b in range(batch_size):\n","        pts = model_points[b].to(pred_R.device)  # [N, 3]\n","\n","        pred_transformed = pred_R[b] @ pts.T + pred_t[b][:, None]  # [3, N]\n","        gt_transformed = gt_R[b] @ pts.T + gt_t[b][:, None]        # [3, N]\n","\n","        dist = torch.norm(pred_transformed - gt_transformed, dim=0)  # [N]\n","        add_loss_b = dist.mean()\n","        add_losses.append(add_loss_b)\n","\n","    return torch.stack(add_losses).mean()\n","\n","\n","def load_ply_vertices(ply_path):\n","    mesh = trimesh.load(ply_path)\n","    return mesh.vertices.astype(np.float32)\n","\n","def set_seed(seed=42):\n","    random.seed(seed)  # Python\n","    np.random.seed(seed)  # NumPy\n","    torch.manual_seed(seed)  # CPU\n","    torch.cuda.manual_seed(seed)  # GPU singola\n","    torch.cuda.manual_seed_all(seed)  # Tutte le GPU\n","\n","    # Comportamento deterministico per reproducibilità\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def normalize_quaternion(q):\n","    return q / q.norm(dim=-1, keepdim=True)\n","\n","def quaternion_to_matrix(q):\n","    # Assumiamo q shape: [B, 4] → ritorna [B, 3, 3]\n","    x, y, z, w = q.unbind(-1)\n","\n","    B = q.size(0)\n","    R = torch.empty(B, 3, 3, device=q.device, dtype=q.dtype)\n","    R[:, 0, 0] = 1 - 2*(y*y + z*z)\n","    R[:, 0, 1] = 2*(x*y - z*w)\n","    R[:, 0, 2] = 2*(x*z + y*w)\n","    R[:, 1, 0] = 2*(x*y + z*w)\n","    R[:, 1, 1] = 1 - 2*(x*x + z*z)\n","    R[:, 1, 2] = 2*(y*z - x*w)\n","    R[:, 2, 0] = 2*(x*z - y*w)\n","    R[:, 2, 1] = 2*(y*z + x*w)\n","    R[:, 2, 2] = 1 - 2*(x*x + y*y)\n","    return R\n","\n","\n","def geodesic_loss(R_pred, R_gt):\n","    # R_pred, R_gt: [B, 3, 3]\n","    R_diff = torch.bmm(R_pred.transpose(1, 2), R_gt)\n","    trace = R_diff[:, 0, 0] + R_diff[:, 1, 1] + R_diff[:, 2, 2]  # batch trace\n","    cos_theta = (trace - 1) / 2\n","    cos_theta = torch.clamp(cos_theta, -1 + 1e-6, 1 - 1e-6)  # stabilità numerica\n","    theta = torch.acos(cos_theta)\n","    return torch.mean(theta)\n","\n","class PreprocessedPoseDataset_RGB(Dataset):\n","    def __init__(self, filenames, rgb_images, pose_data, mean_t, std_t):\n","        self.rgb_images = rgb_images\n","        self.pose_data = pose_data\n","        self.filenames = filenames\n","        self.mean_t = mean_t\n","        self.std_t = std_t\n","\n","        self.obj_id_to_idx = {\n","            1: 0, 2: 1, 4: 2, 5: 3, 6: 4,\n","            8: 5, 9: 6, 10: 7, 11: 8, 12: 9,\n","            13: 10, 14: 11, 15: 12\n","        }\n","\n","    def __len__(self):\n","        return len(self.rgb_images)\n","\n","    def __getitem__(self, idx):\n","        filename = self.filenames[idx]\n","        name_key = os.path.splitext(filename)[0]\n","\n","        rgb = self.rgb_images[idx]\n","        sample = self.pose_data[name_key][0]\n","\n","        rotation = torch.tensor(sample['cam_R_m2c'], dtype=torch.float32).view(3, 3)\n","        translation = torch.tensor(sample['cam_t_m2c'], dtype=torch.float32)\n","        translation = (translation - self.mean_t) / self.std_t  # Normalizzazione\n","\n","        obj_id_raw = sample['obj_id']\n","        obj_id = torch.tensor(self.obj_id_to_idx[obj_id_raw], dtype=torch.long)\n","\n","        return {\n","            'image': rgb,\n","            'cam_R_m2c': rotation,\n","            'cam_t_m2c': translation,\n","            'obj_id': obj_id\n","        }\n","\n","class PoseRegressor_RGB(nn.Module):\n","    def __init__(self, num_obj_ids, embedding_dim=32, use_learned_default=True):\n","        super(PoseRegressor_RGB, self).__init__()\n","\n","        resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n","        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # [B, 2048, 1, 1]\n","\n","        self.obj_id_embedding = nn.Embedding(num_obj_ids, embedding_dim)\n","\n","        # Embedding predefinito (se richiesto)\n","        self.use_learned_default = use_learned_default\n","        if use_learned_default:\n","            self.default_obj_embedding = nn.Parameter(torch.zeros(embedding_dim))\n","\n","        self.fc_common = nn.Sequential(\n","            nn.Linear(2048 + embedding_dim, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(512, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU())\n","\n","        self.fc_translation = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 3))\n","\n","        self.fc_rotation = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 4))\n","\n","    def forward(self, x, obj_id=None):\n","        features = self.feature_extractor(x)  # [B, 2048, 1, 1]\n","        features = features.view(features.size(0), -1)  # [B, 2048]\n","\n","        if obj_id is not None:\n","            obj_embed = self.obj_id_embedding(obj_id)  # [B, D]\n","        else:\n","            if self.use_learned_default:\n","                obj_embed = self.default_obj_embedding.unsqueeze(0).expand(x.size(0), -1)  # [B, D]\n","            else:\n","                obj_embed = torch.zeros(x.size(0), self.obj_id_embedding.embedding_dim, device=x.device)\n","\n","        x = torch.cat([features, obj_embed], dim=1)  # [B, 2048 + D]\n","        x = self.fc_common(x)\n","        translation = self.fc_translation(x)\n","        rotation_q = self.fc_rotation(x)  # [B, 4]\n","        rotation_q = normalize_quaternion(rotation_q)  # normalizza il quaternione\n","        rotation = quaternion_to_matrix(rotation_q)     # converte in matrice 3x3\n","        return translation, rotation\n","\n","if torch.cuda.is_available():\n","  device = \"cuda\"\n","else:\n","  device = \"cpu\"\n","\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["set_seed(42)\n","pose_file = \"/content/drive/MyDrive/DatasetCorrect/gt_all_new.yml\" # Indirizzo della ground truth\n","img_dir = \"/content/drive/MyDrive/DatasetCorrect/dataset_tensors/tensors_rgb.pt\"\n","\n","id_dataset_to_ordered = {1: 0, 2: 1, 4: 2, 5: 3, 6: 4,\n","            8: 5, 9: 6, 10: 7, 11: 8, 12: 9,\n","            13: 10, 14: 11, 15: 12}\n","\n","models_points = {}\n","\n","for i in range(15):\n","  if i != 2 and i != 6:\n","    pts = (load_ply_vertices(f\"/content/drive/MyDrive/DatasetCorrect/normalized_models/obj_norm_{i+1:02d}.ply\"))\n","    models_points[id_dataset_to_ordered[i+1]] = torch.tensor(pts, dtype=torch.float32).to(device)\n","\n","with open(pose_file, 'r') as f:\n","  pose_data = yaml.load(f, Loader=yaml.FullLoader)\n","\n","def compute_translation_stats(pose_data):\n","    translations = []\n","    for v in pose_data.values():\n","        t = np.array(v[0]['cam_t_m2c'])\n","        translations.append(t)\n","    translations = np.stack(translations)\n","    mean = translations.mean(axis=0)\n","    std = translations.std(axis=0)\n","    return mean, std\n","\n","mean_t, std_t = compute_translation_stats(pose_data)\n","mean_t = torch.tensor(mean_t, dtype=torch.float32)\n","std_t = torch.tensor(std_t, dtype=torch.float32)\n","\n","## Creating DataLoader\n","\n","filenames, rgb_tensors = torch.load(img_dir)  # Carica tutti i tensori RGB\n","\n","with open(\"/content/drive/MyDrive/DatasetCorrect/dataset_indexes/train_indexes.txt\", \"r\") as f:\n","    valid_names = set(line.strip() for line in f if line.strip())\n","\n","with open(\"/content/drive/MyDrive/DatasetCorrect/dataset_indexes/val_indexes.txt\", \"r\") as f:\n","    valid_names_val = set(line.strip() for line in f if line.strip())\n","\n","tr_filenames = []\n","val_filenames = []\n","\n","tr_rgb_tensors = []\n","\n","val_rgb_tensors = []\n","\n","for fname, tensor in zip(filenames, rgb_tensors):\n","    name_no_ext = os.path.splitext(fname)[0]\n","    if name_no_ext in valid_names:\n","        tr_filenames.append(fname)\n","        tr_rgb_tensors.append(tensor)\n","    elif name_no_ext in valid_names_val:\n","        val_filenames.append(fname)\n","        val_rgb_tensors.append(tensor)\n","\n","del filenames\n","torch.cuda.empty_cache()\n","\n","dataset = PreprocessedPoseDataset_RGB(tr_filenames, tr_rgb_tensors, pose_data, mean_t, std_t)\n","val_dataset = PreprocessedPoseDataset_RGB(val_filenames, val_rgb_tensors, pose_data, mean_t, std_t)\n","\n","dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2, drop_last=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, drop_last=True)"],"metadata":{"id":"rLuX4U6qVlzu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_seed(42)\n","## ATTENZIONE: PREDIZIONI NORMALIZZATE, VANNO POI RIPORTATE IN MONDO \"REALE\"\n","LR = 0.00001\n","\n","model = PoseRegressor_RGB(num_obj_ids=13).to(device)\n","\n","alpha = 0.07 # Translation Loss coefficient\n","beta = 1 # Rotation Loss coefficient\n","gamma = 1 # ADD Loss Coefficient\n","\n","lr_backbone = 0.00001            # oppure un valore più basso, se vuoi congelare la feature extractor\n","lr_common = 0.00001              # comune per il corpo MLP\n","lr_translation = 0.0003      # es: aumentato per traslazione\n","lr_rotation = 0.00001        # es: lasciato basso per rotazione\n","\n","optimizer = torch.optim.Adam([\n","    {\"params\": model.feature_extractor.parameters(), \"lr\": lr_backbone},\n","    {\"params\": model.fc_common.parameters(), \"lr\": lr_common},\n","    {\"params\": model.fc_translation.parameters(), \"lr\": lr_translation},\n","    {\"params\": model.fc_rotation.parameters(), \"lr\": lr_rotation},], lr=LR)\n","\n","posemodelidrgb = 141 # Scegliere versione modello PoseModel RGB da cui partire\n","n_epochs = 30 # Numero di epoche da trainare\n","\n","# Per ogni parametro del modello, assicurati che i gradienti siano abilitati\n","for param in model.parameters():\n","    param.requires_grad = True\n","\n","if posemodelidrgb > 0:\n","  checkpoint = torch.load(f\"/content/drive/MyDrive/RGB_PoseModels_Correct/posemodel_rgb_{posemodelidrgb}.pth\")\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","\n","optimizer.param_groups[0]['lr'] = lr_backbone\n","optimizer.param_groups[1]['lr'] = lr_common\n","optimizer.param_groups[2]['lr'] = lr_translation\n","optimizer.param_groups[3]['lr'] = lr_rotation\n","\n","\n","for i in range(n_epochs):\n","  model.train()\n","\n","  total_loss = 0.0\n","  total_t_loss = 0.0\n","  total_r_loss = 0.0\n","  total_add_loss = 0.0\n","  total_samples = 0\n","\n","  for batch in dataloader:\n","      batch_size = batch['image'].size(0)\n","      total_samples += batch_size\n","\n","      images = batch['image'].to(device)                 # [B, 3, 224, 224]\n","      gt_t = batch['cam_t_m2c'].to(device).float()               # [B, 3]\n","      gt_r = batch['cam_R_m2c'].to(device).float()               # [B, 3, 3]\n","      obj_ids = batch['obj_id'].to(device)               # [B]\n","\n","      pred_t, pred_r = model(images, obj_ids)\n","\n","      model_points_batch = [models_points[obj_ids[j].item()] for j in range(batch_size)]\n","\n","      t_loss = nn.functional.smooth_l1_loss(pred_t, gt_t, beta=1.0, reduction='mean')\n","      #t_loss = nn.functional.mse_loss(pred_t, gt_t, reduction='mean')\n","      r_loss = geodesic_loss(pred_r, gt_r)\n","      add_loss = compute_add(model_points_batch, gt_r, gt_t, pred_r, pred_t)\n","      loss = gamma * add_loss + alpha * t_loss + beta * r_loss\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      total_loss += loss.item() * batch_size\n","      total_t_loss += t_loss.item() * batch_size\n","      total_r_loss += r_loss.item() * batch_size\n","      total_add_loss += add_loss.item() * batch_size\n","\n","\n","  avg_add_loss = total_add_loss / total_samples\n","  avg_rot_loss = total_r_loss / total_samples\n","  avg_trans_loss = total_t_loss / total_samples\n","\n","  # VALIDATION\n","  model.eval()\n","  val_total_loss = 0.0\n","  val_total_t_loss = 0.0\n","  val_total_r_loss = 0.0\n","  val_total_samples = 0\n","\n","  with torch.no_grad():\n","      for val_batch in val_dataloader:\n","          batch_size = val_batch['image'].size(0)\n","          val_total_samples += batch_size\n","\n","          images = val_batch['image'].to(device)\n","          gt_t = val_batch['cam_t_m2c'].to(device).float()\n","          gt_r = val_batch['cam_R_m2c'].to(device).float()\n","          obj_ids = val_batch['obj_id'].to(device)\n","\n","          pred_t, pred_r = model(images, obj_ids)\n","\n","          model_points_batch = [models_points[obj_ids[j].item()] for j in range(batch_size)]\n","\n","          t_loss = nn.functional.smooth_l1_loss(pred_t, gt_t, beta=1.0, reduction='mean')\n","          r_loss = geodesic_loss(pred_r, gt_r)\n","          loss = compute_add(model_points_batch, gt_r, gt_t, pred_r, pred_t)\n","\n","          val_total_loss += loss.item() * batch_size\n","          val_total_t_loss += t_loss.item() * batch_size\n","          val_total_r_loss += r_loss.item() * batch_size\n","\n","  val_avg_loss = val_total_loss / val_total_samples\n","  val_avg_rot_loss = val_total_r_loss / val_total_samples\n","  val_avg_trans_loss = val_total_t_loss / val_total_samples\n","\n","  torch.save({'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, f\"/content/drive/MyDrive/RGB_PoseModels_Correct/posemodel_rgb_{posemodelidrgb+i+1}.pth\")\n","  print(f\"{i+1}/{n_epochs} -> - Epoch {posemodelidrgb+i+1}\")\n","  print(f\"Average Training ADD (per Sample): {avg_add_loss:.4f} | Avg. Rot. Loss: {avg_rot_loss:.4f} rad / {avg_rot_loss*(180 / torch.pi):.4f}° | Avg. Trans. Loss: {avg_trans_loss:.4f}\" )\n","  print(f\"Average Validation ADD (per Sample): {val_avg_loss:.4f} | Avg. Rot. Loss: {val_avg_rot_loss:.4f} rad / {val_avg_rot_loss*(180 / torch.pi):.4f}° | Avg. Trans. Loss: {val_avg_trans_loss:.4f}\")\n","  print(\"-----------------------------------------------------------------------\")"],"metadata":{"id":"qvU2_4YbVoHB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"50c0b5c4-0be6-44a6-90c3-c0f389a7d6b9"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","100%|██████████| 97.8M/97.8M [00:01<00:00, 77.2MB/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["1/30 -> - Epoch 142\n","Average Training ADD (per Sample): 1.9466 | Avg. Rot. Loss: 0.8902 rad / 51.0019° | Avg. Trans. Loss: 0.4130\n","Average Validation ADD (per Sample): 1.9018 | Avg. Rot. Loss: 0.7976 rad / 45.6996° | Avg. Trans. Loss: 0.4176\n","-----------------------------------------------------------------------\n","2/30 -> - Epoch 143\n","Average Training ADD (per Sample): 1.9423 | Avg. Rot. Loss: 0.8805 rad / 50.4462° | Avg. Trans. Loss: 0.4128\n","Average Validation ADD (per Sample): 1.8997 | Avg. Rot. Loss: 0.7896 rad / 45.2383° | Avg. Trans. Loss: 0.4182\n","-----------------------------------------------------------------------\n","3/30 -> - Epoch 144\n","Average Training ADD (per Sample): 1.9486 | Avg. Rot. Loss: 0.8821 rad / 50.5397° | Avg. Trans. Loss: 0.4149\n","Average Validation ADD (per Sample): 1.8913 | Avg. Rot. Loss: 0.7775 rad / 44.5477° | Avg. Trans. Loss: 0.4170\n","-----------------------------------------------------------------------\n","4/30 -> - Epoch 145\n","Average Training ADD (per Sample): 1.9365 | Avg. Rot. Loss: 0.8709 rad / 49.8967° | Avg. Trans. Loss: 0.4138\n","Average Validation ADD (per Sample): 1.8930 | Avg. Rot. Loss: 0.7756 rad / 44.4365° | Avg. Trans. Loss: 0.4200\n","-----------------------------------------------------------------------\n","5/30 -> - Epoch 146\n","Average Training ADD (per Sample): 1.9451 | Avg. Rot. Loss: 0.8837 rad / 50.6300° | Avg. Trans. Loss: 0.4131\n","Average Validation ADD (per Sample): 1.8998 | Avg. Rot. Loss: 0.7908 rad / 45.3121° | Avg. Trans. Loss: 0.4174\n","-----------------------------------------------------------------------\n","6/30 -> - Epoch 147\n","Average Training ADD (per Sample): 1.9541 | Avg. Rot. Loss: 0.8933 rad / 51.1838° | Avg. Trans. Loss: 0.4141\n","Average Validation ADD (per Sample): 1.8914 | Avg. Rot. Loss: 0.7806 rad / 44.7236° | Avg. Trans. Loss: 0.4175\n","-----------------------------------------------------------------------\n","7/30 -> - Epoch 148\n","Average Training ADD (per Sample): 1.9485 | Avg. Rot. Loss: 0.8841 rad / 50.6542° | Avg. Trans. Loss: 0.4136\n","Average Validation ADD (per Sample): 1.8979 | Avg. Rot. Loss: 0.7893 rad / 45.2216° | Avg. Trans. Loss: 0.4184\n","-----------------------------------------------------------------------\n","8/30 -> - Epoch 149\n","Average Training ADD (per Sample): 1.9410 | Avg. Rot. Loss: 0.8824 rad / 50.5573° | Avg. Trans. Loss: 0.4122\n","Average Validation ADD (per Sample): 1.8971 | Avg. Rot. Loss: 0.7892 rad / 45.2194° | Avg. Trans. Loss: 0.4176\n","-----------------------------------------------------------------------\n","9/30 -> - Epoch 150\n","Average Training ADD (per Sample): 1.9426 | Avg. Rot. Loss: 0.8760 rad / 50.1883° | Avg. Trans. Loss: 0.4157\n","Average Validation ADD (per Sample): 1.8872 | Avg. Rot. Loss: 0.7759 rad / 44.4574° | Avg. Trans. Loss: 0.4174\n","-----------------------------------------------------------------------\n","10/30 -> - Epoch 151\n","Average Training ADD (per Sample): 1.9509 | Avg. Rot. Loss: 0.8931 rad / 51.1712° | Avg. Trans. Loss: 0.4133\n","Average Validation ADD (per Sample): 1.8901 | Avg. Rot. Loss: 0.7779 rad / 44.5707° | Avg. Trans. Loss: 0.4176\n","-----------------------------------------------------------------------\n","11/30 -> - Epoch 152\n","Average Training ADD (per Sample): 1.9464 | Avg. Rot. Loss: 0.8887 rad / 50.9210° | Avg. Trans. Loss: 0.4133\n","Average Validation ADD (per Sample): 1.8908 | Avg. Rot. Loss: 0.7807 rad / 44.7290° | Avg. Trans. Loss: 0.4178\n","-----------------------------------------------------------------------\n","12/30 -> - Epoch 153\n","Average Training ADD (per Sample): 1.9382 | Avg. Rot. Loss: 0.8705 rad / 49.8778° | Avg. Trans. Loss: 0.4143\n","Average Validation ADD (per Sample): 1.8855 | Avg. Rot. Loss: 0.7736 rad / 44.3265° | Avg. Trans. Loss: 0.4171\n","-----------------------------------------------------------------------\n","13/30 -> - Epoch 154\n","Average Training ADD (per Sample): 1.9392 | Avg. Rot. Loss: 0.8843 rad / 50.6660° | Avg. Trans. Loss: 0.4131\n","Average Validation ADD (per Sample): 1.8951 | Avg. Rot. Loss: 0.7861 rad / 45.0420° | Avg. Trans. Loss: 0.4178\n","-----------------------------------------------------------------------\n","14/30 -> - Epoch 155\n","Average Training ADD (per Sample): 1.9511 | Avg. Rot. Loss: 0.8995 rad / 51.5403° | Avg. Trans. Loss: 0.4123\n","Average Validation ADD (per Sample): 1.8817 | Avg. Rot. Loss: 0.7618 rad / 43.6486° | Avg. Trans. Loss: 0.4176\n","-----------------------------------------------------------------------\n","15/30 -> - Epoch 156\n","Average Training ADD (per Sample): 1.9542 | Avg. Rot. Loss: 0.8873 rad / 50.8361° | Avg. Trans. Loss: 0.4155\n","Average Validation ADD (per Sample): 1.8874 | Avg. Rot. Loss: 0.7746 rad / 44.3799° | Avg. Trans. Loss: 0.4179\n","-----------------------------------------------------------------------\n","16/30 -> - Epoch 157\n","Average Training ADD (per Sample): 1.9419 | Avg. Rot. Loss: 0.8873 rad / 50.8402° | Avg. Trans. Loss: 0.4122\n","Average Validation ADD (per Sample): 1.8864 | Avg. Rot. Loss: 0.7728 rad / 44.2761° | Avg. Trans. Loss: 0.4170\n","-----------------------------------------------------------------------\n","17/30 -> - Epoch 158\n","Average Training ADD (per Sample): 1.9467 | Avg. Rot. Loss: 0.8871 rad / 50.8250° | Avg. Trans. Loss: 0.4127\n","Average Validation ADD (per Sample): 1.8800 | Avg. Rot. Loss: 0.7597 rad / 43.5290° | Avg. Trans. Loss: 0.4178\n","-----------------------------------------------------------------------\n","18/30 -> - Epoch 159\n","Average Training ADD (per Sample): 1.9279 | Avg. Rot. Loss: 0.8552 rad / 49.0020° | Avg. Trans. Loss: 0.4135\n","Average Validation ADD (per Sample): 1.8806 | Avg. Rot. Loss: 0.7613 rad / 43.6174° | Avg. Trans. Loss: 0.4183\n","-----------------------------------------------------------------------\n","19/30 -> - Epoch 160\n","Average Training ADD (per Sample): 1.9477 | Avg. Rot. Loss: 0.8828 rad / 50.5798° | Avg. Trans. Loss: 0.4146\n","Average Validation ADD (per Sample): 1.8787 | Avg. Rot. Loss: 0.7616 rad / 43.6365° | Avg. Trans. Loss: 0.4168\n","-----------------------------------------------------------------------\n","20/30 -> - Epoch 161\n","Average Training ADD (per Sample): 1.9443 | Avg. Rot. Loss: 0.8865 rad / 50.7904° | Avg. Trans. Loss: 0.4132\n","Average Validation ADD (per Sample): 1.8738 | Avg. Rot. Loss: 0.7524 rad / 43.1067° | Avg. Trans. Loss: 0.4172\n","-----------------------------------------------------------------------\n","21/30 -> - Epoch 162\n","Average Training ADD (per Sample): 1.9539 | Avg. Rot. Loss: 0.8969 rad / 51.3904° | Avg. Trans. Loss: 0.4133\n","Average Validation ADD (per Sample): 1.8810 | Avg. Rot. Loss: 0.7685 rad / 44.0317° | Avg. Trans. Loss: 0.4164\n","-----------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["os.kill(os.getpid(), 9)"],"metadata":{"id":"QTgkixvpAPxt"},"execution_count":null,"outputs":[]}]}